{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAISS Workflow\n",
    "## 1) create '.txt' files of unique values per attribute based off of the splits files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# This script processes CSV files containing neighbor entries for different attributes,\n",
    "# extracts unique neighbors, and saves them to separate text files for each attribute.\n",
    "\n",
    "# Directory containing the input CSV files\n",
    "directory = 'splits'\n",
    "# Directory where the output text files will be saved\n",
    "output_directory = 'faiss_index/faiss_index_data/individual_values'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# List of attributes to process\n",
    "attributes = ['bioActivity', 'collectionSite', 'collectionSpecie', 'collectionType', 'name']\n",
    "\n",
    "for attribute in attributes:\n",
    "    # Initialize a set to store unique neighbor entries\n",
    "    unique_neighbors = set()\n",
    "\n",
    "    # Iterate over all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.startswith(f\"test_doi_{attribute}\") or filename.startswith(f\"train_doi_{attribute}\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(filepath)\n",
    "            # Add unique neighbor entries to the set\n",
    "            unique_neighbors.update(df['neighbor'].unique())\n",
    "\n",
    "    # Save the unique neighbors to a text file\n",
    "    with open(os.path.join(output_directory, f'unique_{attribute}.txt'), 'w') as f:\n",
    "        for neighbor in sorted(unique_neighbors):\n",
    "            f.write(f\"{neighbor}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) create FAISS indexes based on the unique values in the textfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS indices have been created and saved.\n"
     ]
    }
   ],
   "source": [
    "#create faiss indexes\n",
    "import os\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set your OpenAI API key\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Initialize OpenAI embeddings\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Directory containing the text files\n",
    "input_directory = 'faiss_index/faiss_index_data/individual_values'\n",
    "output_directory = 'faiss_index'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Iterate over all text files in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.txt'):\n",
    "        filepath = os.path.join(input_directory, filename)\n",
    "        \n",
    "        # Read the text file and create Document objects\n",
    "        entities = []\n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f:\n",
    "                text = line.strip()\n",
    "                doc = Document(page_content=text, metadata={'text': text})\n",
    "                entities.append(doc)\n",
    "        \n",
    "        # Create FAISS index from documents\n",
    "        faiss_index = FAISS.from_documents(entities, embeddings)\n",
    "        \n",
    "        # Save the FAISS index locally\n",
    "        index_path = os.path.join(output_directory, f'{filename}.index')\n",
    "        faiss_index.save_local(index_path)\n",
    "\n",
    "print(\"FAISS indices have been created and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5) test the FAISS indexes with similarity search and custom values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test Faiss indexes\n",
    "import os\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "\n",
    "# Set your OpenAI API key\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Initialize OpenAI embeddings\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Directory containing the FAISS indexes\n",
    "index_directory = 'faiss_index'\n",
    "\n",
    "# Mapping from number to attribute\n",
    "attribute_mapping = {\n",
    "    1: 'collectionSpecie',\n",
    "    2: 'collectionSite',\n",
    "    3: 'bioActivity',\n",
    "    4: 'name',\n",
    "    5: 'collectionType'\n",
    "}\n",
    "\n",
    "def load_faiss_index(attribute_number):\n",
    "    attribute = attribute_mapping.get(attribute_number)\n",
    "    if not attribute:\n",
    "        raise ValueError(f\"Invalid attribute number: {attribute_number}\")\n",
    "    index_path = os.path.join(index_directory, f'unique_{attribute}.txt.index')\n",
    "    if not os.path.exists(index_path):\n",
    "        raise FileNotFoundError(f\"FAISS index for attribute '{attribute}' not found.\")\n",
    "    return FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "def similarity_search(attribute_number, query, top_k=5):\n",
    "    faiss_index = load_faiss_index(attribute_number)\n",
    "    docs_with_score = faiss_index.similarity_search_with_score(query, top_k=top_k)\n",
    "    return docs_with_score\n",
    "\n",
    "# Example usage\n",
    "attribute_number = 2  # Change this to the attribute number you want to search (1 to 5)\n",
    "query = \"test\"  # Change this to your query string\n",
    "\n",
    "try:\n",
    "    results = similarity_search(attribute_number, query)\n",
    "    for doc, score in results:\n",
    "        print(f\"Document: {doc.page_content}, Score: {score}\")\n",
    "except (FileNotFoundError, ValueError) as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
