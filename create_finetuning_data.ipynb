{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create JSONL files for Finetuning - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 CSV files in 'splits/train'.\n",
      "Parquet file 'pypdfextraction/extracted_text.parquet' exists.\n",
      "The output directory 'finetune_data' already exists.\n",
      "The output directory 'finetune_data' is empty.\n",
      "Saved fine-tune data for attribute 'name' to finetune_data\\finetune_name_0_1st.jsonl\n",
      "Saved fine-tune data for attribute 'bioActivity' to finetune_data\\finetune_bioActivity_0_1st.jsonl\n",
      "Saved fine-tune data for attribute 'collectionSpecie' to finetune_data\\finetune_collectionSpecie_0_1st.jsonl\n",
      "Saved fine-tune data for attribute 'collectionSite' to finetune_data\\finetune_collectionSite_0_1st.jsonl\n",
      "Saved fine-tune data for attribute 'collectionType' to finetune_data\\finetune_collectionType_0_1st.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory paths\n",
    "splits_dir = 'splits/train'  # Directory where your CSV split files are located\n",
    "parquet_path = 'pypdfextraction/extracted_text.parquet'  # Path to your extracted text parquet file\n",
    "output_dir = 'finetune_data'  # Directory to save fine-tuning data\n",
    "\n",
    "# Check if the directories and files exist\n",
    "if not os.path.exists(splits_dir):\n",
    "    raise FileNotFoundError(f\"The splits directory '{splits_dir}' does not exist.\")\n",
    "else:\n",
    "    # Check for CSV files in the splits directory\n",
    "    csv_files = [f for f in os.listdir(splits_dir) if f.endswith('.csv')]\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No CSV files found in the splits directory '{splits_dir}'.\")\n",
    "    else:\n",
    "        print(f\"Found {len(csv_files)} CSV files in '{splits_dir}'.\")\n",
    "\n",
    "if not os.path.isfile(parquet_path):\n",
    "    raise FileNotFoundError(f\"The parquet file '{parquet_path}' does not exist.\")\n",
    "else:\n",
    "    print(f\"Parquet file '{parquet_path}' exists.\")\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    print(f\"The output directory '{output_dir}' does not exist. Creating it now.\")\n",
    "    os.makedirs(output_dir)\n",
    "else:\n",
    "    print(f\"The output directory '{output_dir}' already exists.\")\n",
    "    # Check if the output directory is empty\n",
    "    if not os.listdir(output_dir):\n",
    "        print(f\"The output directory '{output_dir}' is empty.\")\n",
    "    else:\n",
    "        print(f\"The output directory '{output_dir}' contains files or subdirectories.\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load the extracted text DataFrame\n",
    "extracted_text_df = pd.read_parquet(parquet_path)\n",
    "\n",
    "# Preprocess the 'filename' column in extracted_text_df to match the 'node' format\n",
    "# Remove the '.pdf' extension and replace '@' with '/'\n",
    "extracted_text_df['node'] = extracted_text_df['filename'].str.replace('.pdf', '', regex=False)\n",
    "extracted_text_df['node'] = extracted_text_df['node'].str.replace('@', '/')\n",
    "\n",
    "# Set of filenames in extracted_text_df for quick lookup\n",
    "available_nodes = set(extracted_text_df['node'])\n",
    "\n",
    "# List of attributes and their corresponding prompts and JSON keys\n",
    "attributes_info = {\n",
    "    'name': {\n",
    "        'json_key': 'compoundName',\n",
    "        'prompt': (\n",
    "            \"You are a chemist expert in natural products. \"\n",
    "            \"Extract the compound name from the following text. \"\n",
    "            \"Provide the answer in JSON format: [{\\\"compoundName\\\": \\\"Example Compound Name\\\"}]. \"\n",
    "            \"If the compound name is not specified, leave it empty like \\\"\\\".\"\n",
    "        )\n",
    "    },\n",
    "    'bioActivity': {\n",
    "        'json_key': 'bioactivity',\n",
    "        'prompt': (\n",
    "            \"You are a chemist expert in natural products. \"\n",
    "            \"Extract the bioactivity from the following text. \"\n",
    "            \"Provide the answer in JSON format: [{\\\"bioactivity\\\": \\\"Example Bioactivity\\\"}]. \"\n",
    "            \"If the bioactivity is not specified, leave it empty like \\\"\\\".\"\n",
    "        )\n",
    "    },\n",
    "    'collectionSpecie': {\n",
    "        'json_key': 'species',\n",
    "        'prompt': (\n",
    "            \"You are a chemist expert in natural products. \"\n",
    "            \"Extract the species from the following text. \"\n",
    "            \"Provide the answer in JSON format: [{\\\"species\\\": \\\"Example Species\\\"}]. \"\n",
    "            \"If the species is not specified, leave it empty like \\\"\\\".\"\n",
    "        )\n",
    "    },\n",
    "    'collectionSite': {\n",
    "        'json_key': 'collectionSite',\n",
    "        'prompt': (\n",
    "            \"You are a chemist expert in natural products. \"\n",
    "            \"Extract the collection site from the following text. \"\n",
    "            \"Provide the answer in JSON format: [{\\\"collectionSite\\\": \\\"Example Collection Site\\\"}]. \"\n",
    "            \"If the collection site is not specified, leave it empty like \\\"\\\".\"\n",
    "        )\n",
    "    },\n",
    "    'collectionType': {\n",
    "        'json_key': 'isolationType',\n",
    "        'prompt': (\n",
    "            \"You are a chemist expert in natural products. \"\n",
    "            \"Extract the isolation type from the following text. \"\n",
    "            \"Provide the answer in JSON format: [{\\\"isolationType\\\": \\\"Example Isolation Type\\\"}]. \"\n",
    "            \"If the isolation type is not specified, leave it empty like \\\"\\\".\"\n",
    "        )\n",
    "    }\n",
    "}\n",
    "\n",
    "# Iteration and split to use\n",
    "iteration = '0'\n",
    "split = '1st'\n",
    "\n",
    "for attribute, info in attributes_info.items():\n",
    "    # Construct the filename for the CSV file\n",
    "    csv_filename = f'train_doi_{attribute}_{iteration}_{split}.csv'\n",
    "    csv_path = os.path.join(splits_dir, csv_filename)\n",
    "\n",
    "    # Check if the CSV file exists\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"File {csv_path} does not exist. Skipping attribute '{attribute}'.\")\n",
    "        continue\n",
    "\n",
    "    # Load the CSV file\n",
    "    csv_df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Preprocess the 'node' column in csv_df to match the 'node' in extracted_text_df\n",
    "    csv_df['node'] = csv_df['node'].str.replace('@', '/')  # Replace '@' with '/'\n",
    "\n",
    "    # Filter out rows where the node doesn't have a corresponding PDF text\n",
    "    csv_df = csv_df[csv_df['node'].isin(available_nodes)]\n",
    "\n",
    "    if csv_df.empty:\n",
    "        print(f\"No matching nodes found for attribute '{attribute}'. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Merge with the extracted text DataFrame on the 'node'\n",
    "    merged_df = pd.merge(csv_df, extracted_text_df[['node', 'text']], on='node')\n",
    "\n",
    "    finetune_data = []\n",
    "\n",
    "    # System prompt for the current attribute\n",
    "    system_prompt = info['prompt']\n",
    "    json_key = info['json_key']\n",
    "\n",
    "    for _, row in merged_df.iterrows():\n",
    "        user_input = row['text']  # Text extracted from the PDF\n",
    "\n",
    "        # Prepare the expected output\n",
    "        expected_output = {json_key: row['neighbor'] if pd.notna(row['neighbor']) else \"\"}\n",
    "\n",
    "        # Create the message object\n",
    "        message = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_input},\n",
    "                {\"role\": \"assistant\", \"content\": json.dumps([expected_output], ensure_ascii=False)}\n",
    "            ]\n",
    "        }\n",
    "        finetune_data.append(message)\n",
    "\n",
    "    if not finetune_data:\n",
    "        print(f\"No data to write for attribute '{attribute}'. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Save to JSONL file\n",
    "    output_filename = f'finetune_{attribute}_{iteration}_{split}.jsonl'\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f_out:\n",
    "        for entry in finetune_data:\n",
    "            json_line = json.dumps(entry, ensure_ascii=False)\n",
    "            f_out.write(json_line + '\\n')\n",
    "\n",
    "    print(f\"Saved fine-tune data for attribute '{attribute}' to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create JSONL files for Finetuning - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 CSV files in 'splits'.\n",
      "Parquet file 'pypdfextraction/extracted_text.parquet' exists.\n",
      "The output directory 'finetune_data' already exists.\n",
      "The output directory 'finetune_data' contains files or subdirectories.\n",
      "Saved fine-tune test data for attribute 'name' to finetune_data/finetune_test_doi_name_0_1st.jsonl\n",
      "Saved fine-tune test data for attribute 'bioActivity' to finetune_data/finetune_test_doi_bioActivity_0_1st.jsonl\n",
      "Saved fine-tune test data for attribute 'collectionSpecie' to finetune_data/finetune_test_doi_collectionSpecie_0_1st.jsonl\n",
      "Saved fine-tune test data for attribute 'collectionSite' to finetune_data/finetune_test_doi_collectionSite_0_1st.jsonl\n",
      "Saved fine-tune test data for attribute 'collectionType' to finetune_data/finetune_test_doi_collectionType_0_1st.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory paths\n",
    "splits_dir = 'splits'  # Changed from 'splits/train' to just 'splits'\n",
    "parquet_path = 'pypdfextraction/extracted_text.parquet'  # Path to your extracted text parquet file\n",
    "output_dir = 'finetune_data'  # Directory to save fine-tuning data\n",
    "\n",
    "# Check if the directories and files exist\n",
    "if not os.path.exists(splits_dir):\n",
    "    raise FileNotFoundError(f\"The splits directory '{splits_dir}' does not exist.\")\n",
    "else:\n",
    "    # Check for CSV files in the splits directory\n",
    "    csv_files = [f for f in os.listdir(splits_dir) if f.endswith('.csv')]\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No CSV files found in the splits directory '{splits_dir}'.\")\n",
    "    else:\n",
    "        print(f\"Found {len(csv_files)} CSV files in '{splits_dir}'.\")\n",
    "\n",
    "if not os.path.isfile(parquet_path):\n",
    "    raise FileNotFoundError(f\"The parquet file '{parquet_path}' does not exist.\")\n",
    "else:\n",
    "    print(f\"Parquet file '{parquet_path}' exists.\")\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    print(f\"The output directory '{output_dir}' does not exist. Creating it now.\")\n",
    "    os.makedirs(output_dir)\n",
    "else:\n",
    "    print(f\"The output directory '{output_dir}' already exists.\")\n",
    "    # Check if the output directory is empty\n",
    "    if not os.listdir(output_dir):\n",
    "        print(f\"The output directory '{output_dir}' is empty.\")\n",
    "    else:\n",
    "        print(f\"The output directory '{output_dir}' contains files or subdirectories.\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load the extracted text DataFrame\n",
    "extracted_text_df = pd.read_parquet(parquet_path)\n",
    "\n",
    "# Preprocess the 'filename' column in extracted_text_df to match the 'node' format\n",
    "# Remove the '.pdf' extension and replace '@' with '/'\n",
    "extracted_text_df['node'] = extracted_text_df['filename'].str.replace('.pdf', '', regex=False)\n",
    "extracted_text_df['node'] = extracted_text_df['node'].str.replace('@', '/')\n",
    "\n",
    "# Set of filenames in extracted_text_df for quick lookup\n",
    "available_nodes = set(extracted_text_df['node'])\n",
    "\n",
    "# List of attributes and their corresponding prompts and JSON keys\n",
    "attributes_info = {\n",
    "    'name': {\n",
    "        'json_key': 'compoundName',\n",
    "        'prompt': (\n",
    "            \"You are a chemist expert in natural products. \"\n",
    "            \"Extract the compound name from the following text. \"\n",
    "            \"Provide the answer in JSON format: [{\\\"compoundName\\\": \\\"Example Compound Name\\\"}]. \"\n",
    "            \"If the compound name is not specified, leave it empty like \\\"\\\".\"\n",
    "        )\n",
    "    },\n",
    "    'bioActivity': {\n",
    "        'json_key': 'bioactivity',\n",
    "        'prompt': (\n",
    "            \"You are a chemist expert in natural products. \"\n",
    "            \"Extract the bioactivity from the following text. \"\n",
    "            \"Provide the answer in JSON format: [{\\\"bioactivity\\\": \\\"Example Bioactivity\\\"}]. \"\n",
    "            \"If the bioactivity is not specified, leave it empty like \\\"\\\".\"\n",
    "        )\n",
    "    },\n",
    "    'collectionSpecie': {\n",
    "        'json_key': 'species',\n",
    "        'prompt': (\n",
    "            \"You are a chemist expert in natural products. \"\n",
    "            \"Extract the species from the following text. \"\n",
    "            \"Provide the answer in JSON format: [{\\\"species\\\": \\\"Example Species\\\"}]. \"\n",
    "            \"If the species is not specified, leave it empty like \\\"\\\".\"\n",
    "        )\n",
    "    },\n",
    "    'collectionSite': {\n",
    "        'json_key': 'collectionSite',\n",
    "        'prompt': (\n",
    "            \"You are a chemist expert in natural products. \"\n",
    "            \"Extract the collection site from the following text. \"\n",
    "            \"Provide the answer in JSON format: [{\\\"collectionSite\\\": \\\"Example Collection Site\\\"}]. \"\n",
    "            \"If the collection site is not specified, leave it empty like \\\"\\\".\"\n",
    "        )\n",
    "    },\n",
    "    'collectionType': {\n",
    "        'json_key': 'isolationType',\n",
    "        'prompt': (\n",
    "            \"You are a chemist expert in natural products. \"\n",
    "            \"Extract the isolation type from the following text. \"\n",
    "            \"Provide the answer in JSON format: [{\\\"isolationType\\\": \\\"Example Isolation Type\\\"}]. \"\n",
    "            \"If the isolation type is not specified, leave it empty like \\\"\\\".\"\n",
    "        )\n",
    "    }\n",
    "}\n",
    "\n",
    "# Iteration and split to use\n",
    "iteration = '0'\n",
    "split = '1st'\n",
    "\n",
    "for attribute, info in attributes_info.items():\n",
    "    # Construct the filename for the CSV file\n",
    "    csv_filename = f'test_doi_{attribute}_{iteration}_{split}.csv'  # Changed from 'train_doi_' to 'test_doi_'\n",
    "    csv_path = os.path.join(splits_dir, csv_filename)\n",
    "\n",
    "    # Check if the CSV file exists\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"File {csv_path} does not exist. Skipping attribute '{attribute}'.\")\n",
    "        continue\n",
    "\n",
    "    # Load the CSV file\n",
    "    csv_df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Preprocess the 'node' column in csv_df to match the 'node' in extracted_text_df\n",
    "    csv_df['node'] = csv_df['node'].str.replace('@', '/')  # Replace '@' with '/'\n",
    "\n",
    "    # Filter out rows where the node doesn't have a corresponding PDF text\n",
    "    csv_df = csv_df[csv_df['node'].isin(available_nodes)]\n",
    "\n",
    "    if csv_df.empty:\n",
    "        print(f\"No matching nodes found for attribute '{attribute}'. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Merge with the extracted text DataFrame on the 'node'\n",
    "    merged_df = pd.merge(csv_df, extracted_text_df[['node', 'text']], on='node')\n",
    "\n",
    "    finetune_data = []\n",
    "\n",
    "    # System prompt for the current attribute\n",
    "    system_prompt = info['prompt']\n",
    "    json_key = info['json_key']\n",
    "\n",
    "    for _, row in merged_df.iterrows():\n",
    "        user_input = row['text']  # Text extracted from the PDF\n",
    "\n",
    "        # Prepare the expected output\n",
    "        expected_output = {json_key: row['neighbor'] if pd.notna(row['neighbor']) else \"\"}\n",
    "\n",
    "        # Create the message object\n",
    "        message = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_input},\n",
    "                {\"role\": \"assistant\", \"content\": json.dumps([expected_output], ensure_ascii=False)}\n",
    "            ]\n",
    "        }\n",
    "        finetune_data.append(message)\n",
    "\n",
    "    if not finetune_data:\n",
    "        print(f\"No data to write for attribute '{attribute}'. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Save to JSONL file\n",
    "    output_filename = f'finetune_test_doi_{attribute}_{iteration}_{split}.jsonl'  # Changed to include 'test_doi_'\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f_out:\n",
    "        for entry in finetune_data:\n",
    "            json_line = json.dumps(entry, ensure_ascii=False)\n",
    "            f_out.write(json_line + '\\n')\n",
    "\n",
    "    print(f\"Saved fine-tune test data for attribute '{attribute}' to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate JSONL files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating finetune_data/finetune_test_doi_collectionSpecie_0_1st.jsonl\n",
      "No errors found in finetune_data/finetune_test_doi_collectionSpecie_0_1st.jsonl\n",
      "\n",
      "Validating finetune_data/finetune_collectionSite_0_1st.jsonl\n",
      "No errors found in finetune_data/finetune_collectionSite_0_1st.jsonl\n",
      "\n",
      "Validating finetune_data/finetune_bioActivity_0_1st.jsonl\n",
      "No errors found in finetune_data/finetune_bioActivity_0_1st.jsonl\n",
      "\n",
      "Validating finetune_data/finetune_collectionSpecie_0_1st.jsonl\n",
      "No errors found in finetune_data/finetune_collectionSpecie_0_1st.jsonl\n",
      "\n",
      "Validating finetune_data/finetune_name_0_1st.jsonl\n",
      "No errors found in finetune_data/finetune_name_0_1st.jsonl\n",
      "\n",
      "Validating finetune_data/finetune_collectionType_0_1st.jsonl\n",
      "No errors found in finetune_data/finetune_collectionType_0_1st.jsonl\n",
      "\n",
      "Validating finetune_data/finetune_test_doi_bioActivity_0_1st.jsonl\n",
      "No errors found in finetune_data/finetune_test_doi_bioActivity_0_1st.jsonl\n",
      "\n",
      "Validating finetune_data/finetune_test_doi_collectionType_0_1st.jsonl\n",
      "No errors found in finetune_data/finetune_test_doi_collectionType_0_1st.jsonl\n",
      "\n",
      "Validating finetune_data/finetune_test_doi_name_0_1st.jsonl\n",
      "No errors found in finetune_data/finetune_test_doi_name_0_1st.jsonl\n",
      "\n",
      "Validating finetune_data/finetune_test_doi_collectionSite_0_1st.jsonl\n",
      "No errors found in finetune_data/finetune_test_doi_collectionSite_0_1st.jsonl\n",
      "\n",
      "Validation complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from jsonschema import validate\n",
    "from jsonschema.exceptions import ValidationError\n",
    "\n",
    "# Define the schema\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"messages\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"role\": {\"type\": \"string\", \"enum\": [\"system\", \"user\", \"assistant\"]},\n",
    "                    \"content\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"required\": [\"role\", \"content\"]\n",
    "            },\n",
    "            \"minItems\": 3,\n",
    "            \"maxItems\": 3\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"messages\"]\n",
    "}\n",
    "\n",
    "def validate_jsonl(file_path):\n",
    "    print(f\"Validating {file_path}\")\n",
    "    errors = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line_number, line in enumerate(file, 1):\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                \n",
    "                # Validate against schema\n",
    "                validate(instance=data, schema=schema)\n",
    "                \n",
    "                # Additional custom checks\n",
    "                messages = data['messages']\n",
    "                \n",
    "                # Check system message\n",
    "                assert messages[0]['role'] == 'system', f\"First message must have role 'system' on line {line_number}\"\n",
    "                assert \"You are a chemist expert in natural products.\" in messages[0]['content'], f\"Incorrect system prompt on line {line_number}\"\n",
    "                \n",
    "                # Check user message\n",
    "                assert messages[1]['role'] == 'user', f\"Second message must have role 'user' on line {line_number}\"\n",
    "                \n",
    "                # Check assistant message\n",
    "                assert messages[2]['role'] == 'assistant', f\"Third message must have role 'assistant' on line {line_number}\"\n",
    "                assistant_content = json.loads(messages[2]['content'])\n",
    "                assert isinstance(assistant_content, list) and len(assistant_content) == 1, f\"Assistant content must be a list with one item on line {line_number}\"\n",
    "                assert len(assistant_content[0]) == 1, f\"Assistant content must have exactly one key-value pair on line {line_number}\"\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                errors.append(f\"Invalid JSON on line {line_number}\")\n",
    "            except ValidationError as e:\n",
    "                errors.append(f\"Schema validation error on line {line_number}: {e}\")\n",
    "            except AssertionError as e:\n",
    "                errors.append(f\"Custom validation error on line {line_number}: {e}\")\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"Errors found in {file_path}:\")\n",
    "        for error in errors:\n",
    "            print(f\"  - {error}\")\n",
    "    else:\n",
    "        print(f\"No errors found in {file_path}\")\n",
    "    print()  # Add a blank line for readability\n",
    "\n",
    "# Directory containing the JSONL files\n",
    "finetune_data_dir = 'finetune_data'\n",
    "\n",
    "# Validate all JSONL files in the directory\n",
    "for filename in os.listdir(finetune_data_dir):\n",
    "    if filename.endswith('.jsonl'):\n",
    "        file_path = os.path.join(finetune_data_dir, filename)\n",
    "        validate_jsonl(file_path)\n",
    "\n",
    "print(\"Validation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
