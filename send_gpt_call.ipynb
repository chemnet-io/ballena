{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nbstripout is a tool to remove the output from Jupyter notebooks\n",
    "#!nbstripout --install\n",
    "!export PYTHONWARNINGS=\"ignore:NotOpenSSLWarning\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openai_api(path):\n",
    "    k = 1\n",
    "    global response_string, response_res, json_array_one\n",
    "    json_array_one = []\n",
    "    loader = PyPDFLoader(path)\n",
    "    pages = loader.load_and_split()\n",
    "    system_message = (\n",
    "        \"In this scenario, you are a chemist with a focus on natural products, tasked with analyzing a scientific paper to identify a specific molecule. Your response must be structured in JSON format, capturing key information about the molecules in question. This includes: `IUPAC_nomenclature`: This field represents the molecule's name according to the International Union of Pure and Applied Chemistry's systematic naming conventions. For example, \\\"3,4-dihydroxybenzoic acid geranyl ester\\\" clearly describes the chemical structure of the compound in a standardized way. `bioActivity`: Here, you detail the molecule's biological effect or function. \\\"Inhibition of Protease\\\" means the compound prevents or reduces the activity of protease enzymes, crucial for understanding its potential therapeutic uses. `collectionSpecies`: This specifies the biological source or species from which the molecule was isolated, such as \\\"Piper crassinervium (Piperaceae)\\\", pointing to a specific plant within the Piperaceae family. `collectionSite`: Indicates the geographical origin where the compound was collected or the organism was found. \\\"Araraquara/SP\\\" refers to a location in SÃ£o Paulo, Brazil, providing context for the environmental conditions of the source. `collectionType`: Describes the origin or process through which the compound was obtained, such as \\\"Biotransformation Product\\\", indicating the compound results from a biological organism chemically modifying a precursor compound. Your analysis should be precise, adhering to the JSON format provided, with each field filled according to the information available from the paper. If certain details are not mentioned, leave the fields empty with `\"\"`. If there are multiple molecules, provide information for each one separately. The keys are the same for each molecule, but the values will differ based on the information available in the document. Ensure that the JSON format is maintained for each molecule. Name the molecules as Molecule_1, Molecule_2, and so on. Please find as much information as possible about each molecule in the document.\"\n",
    "    )\n",
    "\n",
    "    client = OpenAI()\n",
    "    client.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "    response_list = \"\"\n",
    "    response_res = \"\"\n",
    "\n",
    "    for i in range(0, k):\n",
    "        user_message = (\n",
    "            f\"Based on the document content: {pages}, and previous analyses: {response_list} but check if its right otherwise change it, identify the described molecule enrich it with information from the above. If specific information is not available, leave the field empty.\"\n",
    "        )\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "            seed=920987036854775807,\n",
    "            stream=True,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            model=\"gpt-4o\",\n",
    "        )\n",
    "\n",
    "        response_string = \"\"\n",
    "        for chunk in response:\n",
    "            json_array_one.append(chunk.choices[0].delta.content or \"\")\n",
    "            response_res += chunk.choices[0].delta.content or \"\"\n",
    "            response_string += chunk.choices[0].delta.content or \"\"\n",
    "            #print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
    "            response_list = response_list + str(chunk.choices[0].delta.content) or \"\"\n",
    "\n",
    "    return response_res\n",
    "\n",
    "\n",
    "\n",
    "pdf = \"pdfs/10.1016@0031-9422(73)85034-4.pdf\"\n",
    "df2 = pd.DataFrame(columns=[\"pdf\", \"output\"])\n",
    "\n",
    "stream = call_openai_api(pdf)\n",
    "stream = stream.replace(\"\\n\", \"\")\n",
    "df2 = df2._append({\"pdf\": pdf, \"output\": stream}, ignore_index=True)\n",
    "\n",
    "df2.to_csv(pdf + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def process_pdfs_in_folder(folder_path):\n",
    "    # Initialize an empty DataFrame for consolidated data\n",
    "    consolidated_df = pd.DataFrame(columns=[\"pdf\", \"output\"])\n",
    "    i =0\n",
    "    # List all PDF files in the specified folder\n",
    "    pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "\n",
    "    # Process each PDF file\n",
    "    for pdf_file in pdf_files:\n",
    "        i += 1\n",
    "        print(f\"Processing PDF {i}/{len(pdf_files)}: {pdf_file}\")\n",
    "        pdf_path = os.path.join(folder_path, pdf_file)\n",
    "        try:\n",
    "            stream = call_openai_api(pdf_path)\n",
    "            stream = stream.replace(\"\\n\", \"\")\n",
    "            # Create a DataFrame for the current PDF\n",
    "            df_current = pd.DataFrame([{\"pdf\": pdf_path, \"output\": stream}])\n",
    "            # Append the current DataFrame to the consolidated DataFrame\n",
    "            consolidated_df = pd.concat([consolidated_df, df_current], ignore_index=True)\n",
    "            # Export the current DataFrame to an individual CSV file\n",
    "            csv_filename = os.path.join(folder_path, pdf_file + \".csv\")\n",
    "            df_current.to_csv(csv_filename, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_path}: {e}\")\n",
    "            # sleep for 60 seconds to avoid hitting the OpenAI API rate limit\n",
    "            time.sleep(60)\n",
    "\n",
    "    # Export the consolidated DataFrame to a CSV file\n",
    "    consolidated_csv_filename = os.path.join(folder_path, \"consolidated_output.csv\")\n",
    "    consolidated_df.to_csv(consolidated_csv_filename, index=False)\n",
    "\n",
    "\n",
    "# Process PDFs in the specified folder\n",
    "process_pdfs_in_folder('pdfs/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_df = pd.read_csv('consolidated_output_full.csv')\n",
    "#print(consolidated_df)\n",
    "# iterate over the row pdf of the DataFrame and search if the pdfs of the folder are in the row pdf\n",
    "# if they are not, then print the pdf name and add them to the list of pdfs not processed\n",
    "not_processed_pdfs = []\n",
    "for pdf in os.listdir('pdfs/'):\n",
    "    if pdf.endswith('.pdf'):\n",
    "        if \"pdfs/\"+pdf not in consolidated_df['pdf'].values:\n",
    "            print(f\"PDF {pdf} not processed.\")\n",
    "            not_processed_pdfs.append(pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call again for the pdfs not processed and add them to the consolidated_df\n",
    "for pdf in not_processed_pdfs:\n",
    "    pdf_path = os.path.join('pdfs/', pdf)\n",
    "    try:\n",
    "        stream = call_openai_api(pdf_path)\n",
    "        stream = stream.replace(\"\\n\", \"\")\n",
    "        df_current = pd.DataFrame([{\"pdf\": pdf_path, \"output\": stream}])\n",
    "        consolidated_df = pd.concat([consolidated_df, df_current], ignore_index=True)\n",
    "        csv_filename = os.path.join('pdfs/', pdf + \".csv\")\n",
    "        df_current.to_csv(csv_filename, index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {e}\")\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the consolidated_df to a csv file\n",
    "consolidated_df.to_csv('consolidated_output_full.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
